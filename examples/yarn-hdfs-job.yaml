# Format for cronjob:
#
# apiVersion: batch/v1beta1
# kind: CronJob
# metadata:
#   name: hdfs-etl
# spec:
#   schedule: "* * * * *" # every minute
#   concurrencyPolicy: Forbid
#   jobTemplate:
#     spec:
#       template:
#         spec:
#         ....
#

apiVersion: batch/v1
kind: Job
metadata:
  name: hdfs-etl
  namespace: default
spec:
  template:
    metadata:
      labels:
        app: hdfs-etl
    spec:
      containers:
      - name: hdfs-etl
          # take image with spark configuration release from 26.04.2019
          # if to pull new image each time: imagePullPolicy: Always
        image: gitlab-registry.cern.ch/db/spark-service/spark-k8s-examples:v1.0-020519
          # source configuration and submit job
        command: ["/bin/sh"]
        args:
        - -c
        - source hadoop-setconf.sh analytix; spark-submit --conf spark.driver.host=$POD_HOSTNAME --class ch.cern.monitoring.HdfsETL $SPARK_HOME/examples/jars/spark-k8s-examples-assembly-1.0.jar;
        env:
          # authenticate yarn/hdfs access
        - name: KRB5CCNAME
          value: "/etc/krb5/krb5cc"
          # define driver hostname
        - name: POD_HOSTNAME
          valueFrom:
            fieldRef:
              fieldPath: status.hostIP
        volumeMounts:
          # authenticate yarn/hdfs access
        - name: krb5
          readOnly: true
          mountPath: "/etc/krb5"
      restartPolicy: OnFailure
      volumes:
        # authenticate yarn/hdfs access
      - name: krb5
        secret:
          secretName: krb5
        # required for access to the driver host ports from yarn cluster
      hostNetwork: true